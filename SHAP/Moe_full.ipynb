{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413e43b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- This assumes your model files (moe_model.py, mop_model.py) are in the same directory ---\n",
    "from moe_model import MoE as MoE_raw, MLP as MoE_Expert\n",
    "from mop_model import MoPModel as MoP_raw, MoPConfig\n",
    "\n",
    "# --- Helper class to modify the MoE Expert to output raw logits ---\n",
    "class MoE_Expert_Logits(MoE_Expert):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super().__init__(input_size, output_size, hidden_size)\n",
    "        self.soft = nn.Identity()\n",
    "\n",
    "print(\"Block 1: Imports and setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cced59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads a dataset, splits it into train/val/test sets, and scales the features.\n",
    "    This ensures the exact same data split is used as during training.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”¹ Loading data from '{file_path}'...\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    X = df.drop(columns=['Dementia Status'])\n",
    "    y = df['Dementia Status']\n",
    "    \n",
    "    # Store feature names for later use\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Replicate the exact data split you used for training\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "    \n",
    "    # Scale the data in the same way\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"Data loading, splitting, and scaling complete.\")\n",
    "    return X_train_scaled, X_test_scaled, feature_names\n",
    "\n",
    "print(\"Block 2: Data loading function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f52bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_shap_analysis(model_type, model_params, model_path, background_data, test_data, feature_names):\n",
    "    \"\"\"\n",
    "    Loads a trained PyTorch model, performs SHAP analysis, and displays the summary plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*40}\\nðŸš€ Starting SHAP Analysis for: {model_path}\\n{'='*40}\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # 1. Re-create the model with the winning architecture\n",
    "    if model_type == 'MoP':\n",
    "        config = MoPConfig(**model_params)\n",
    "        model = MoP_raw(config)\n",
    "    else: # MoE\n",
    "        model = MoE_raw(**model_params)\n",
    "        model.experts = nn.ModuleList([\n",
    "            MoE_Expert_Logits(\n",
    "                input_size=model_params['input_size'],\n",
    "                output_size=2,\n",
    "                hidden_size=model_params['hidden_size']\n",
    "            ) for _ in range(model.num_experts)\n",
    "        ])\n",
    "    \n",
    "    # 2. Load the saved weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"Model and weights loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ERROR: Model file not found at '{model_path}'. Please check the path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR: Failed to load model weights. Ensure the model parameters below match the saved model.\")\n",
    "        print(f\"   Parameters used: {model_params}\")\n",
    "        print(f\"   Original error: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Prepare data and wrapper for SHAP\n",
    "    background_tensor = torch.from_numpy(background_data).float().to(device)\n",
    "    test_tensor = torch.from_numpy(test_data).float().to(device)\n",
    "\n",
    "    # SHAP needs a function that returns raw model outputs (logits)\n",
    "    def model_prediction_wrapper(x):\n",
    "        if model_type == 'MoP':\n",
    "            # Add sequence dimension for MoP\n",
    "            pred, _, _ = model(x.unsqueeze(1))\n",
    "            return pred.squeeze(1)\n",
    "        else: # MoE\n",
    "            pred, _ = model(x)\n",
    "            return pred\n",
    "\n",
    "    # 4. Create the SHAP explainer and calculate values\n",
    "    # We use a sample of the training data as the background distribution\n",
    "    explainer = shap.DeepExplainer(model_prediction_wrapper, background_tensor)\n",
    "    \n",
    "    print(\"Calculating SHAP values... (This may take a moment)\")\n",
    "    # We calculate SHAP values on the test set to understand predictions on unseen data\n",
    "    shap_values = explainer.shap_values(test_tensor)\n",
    "    \n",
    "    # 5. Generate and display the summary plot\n",
    "    print(\"\\n--- SHAP Summary Plot ---\")\n",
    "    # For binary classification, we plot the SHAP values for the positive class (class 1)\n",
    "    shap.summary_plot(shap_values[1], features=test_data, feature_names=feature_names)\n",
    "\n",
    "print(\"Block 3: SHAP analysis function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- 1. Load both datasets ---\n",
    "    X_train_full, X_test_full, feature_names_full = load_and_prepare_data('uk_biobank_dataset.csv')\n",
    "    X_train_noninvasive, X_test_noninvasive, feature_names_noninvasive = load_and_prepare_data('input_data_noninvasive.csv')\n",
    "\n",
    "    # --- 2. Define the BEST hyperparameters found for each model ---\n",
    "    # âš ï¸ IMPORTANT: YOU MUST UPDATE THESE DICTIONARIES WITH YOUR WINNING PARAMETERS!\n",
    "    \n",
    "    best_params_mop_full = {\n",
    "        'input_dim': X_train_full.shape[1], 'output_dim': 2,\n",
    "        'intermediate_dim': 32, 'layers': [\"0,16,32\", \"0,8,16\", \"0,16,32\"] # Example\n",
    "    }\n",
    "    \n",
    "    best_params_moe_full = {\n",
    "        'input_size': X_train_full.shape[1], 'output_size': 2,\n",
    "        'num_experts': 6, 'hidden_size': 32, 'k': 4 # Example\n",
    "    }\n",
    "    \n",
    "    best_params_mop_noninvasive = {\n",
    "        'input_dim': X_train_noninvasive.shape[1], 'output_dim': 2,\n",
    "        'intermediate_dim': 64, 'layers': [\"0,8,16\", \"0,8,16\"] # Example\n",
    "    }\n",
    "\n",
    "    best_params_moe_noninvasive = {\n",
    "        'input_size': X_train_noninvasive.shape[1], 'output_size': 2,\n",
    "        'num_experts': 8, 'hidden_size': 16, 'k': 3 # Example\n",
    "    }\n",
    "\n",
    "    # --- 3. Run SHAP analysis for each model ---\n",
    "    \n",
    "    # Analysis for MoP on the full dataset\n",
    "    perform_shap_analysis('MoP', best_params_mop_full, 'best_mop_fulldataset.pth',\n",
    "                          X_train_full, X_test_full, feature_names_full)\n",
    "                          \n",
    "    # Analysis for MoE on the full dataset\n",
    "    perform_shap_analysis('MoE', best_params_moe_full, 'best_moe_fulldataset.pth',\n",
    "                          X_train_full, X_test_full, feature_names_full)\n",
    "\n",
    "    # Analysis for MoP on the non-invasive dataset\n",
    "    perform_shap_analysis('MoP', best_params_mop_noninvasive, 'best_mop_noninvasivedataset.pth',\n",
    "                          X_train_noninvasive, X_test_noninvasive, feature_names_noninvasive)\n",
    "                          \n",
    "    # Analysis for MoE on the non-invasive dataset\n",
    "    # Note: You mentioned two MoE files for non-invasive, I assume one was a typo. Using 'best_moe_noninvasivedataset.pth'\n",
    "    perform_shap_analysis('MoE', best_params_moe_noninvasive, 'best_moe_noninvasivedataset.pth',\n",
    "                          X_train_noninvasive, X_test_noninvasive, feature_names_noninvasive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab1fa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.44.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (538 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 538 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging>20.9\n",
      "  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66 kB 3.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cloudpickle\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pandas in /home/user/.local/lib/python3.8/site-packages (from shap) (2.0.3)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy in /home/user/.local/lib/python3.8/site-packages (from shap) (1.10.1)\n",
      "Collecting numba\n",
      "  Downloading numba-0.58.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.7 MB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/user/.local/lib/python3.8/site-packages (from shap) (1.3.2)\n",
      "Collecting tqdm>=4.27.0\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78 kB 5.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/user/.local/lib/python3.8/site-packages (from shap) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user/.local/lib/python3.8/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user/.local/lib/python3.8/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/user/.local/lib/python3.8/site-packages (from pandas->shap) (2025.2)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0\n",
      "  Downloading llvmlite-0.41.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43.6 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.9\" in /home/user/.local/lib/python3.8/site-packages (from numba->shap) (8.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/.local/lib/python3.8/site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/user/.local/lib/python3.8/site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/user/.local/lib/python3.8/site-packages (from importlib-metadata; python_version < \"3.9\"->numba->shap) (3.20.2)\n",
      "Installing collected packages: packaging, cloudpickle, slicer, llvmlite, numba, tqdm, shap\n",
      "Successfully installed cloudpickle-3.1.1 llvmlite-0.41.1 numba-0.58.1 packaging-25.0 shap-0.44.1 slicer-0.0.7 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install shap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
