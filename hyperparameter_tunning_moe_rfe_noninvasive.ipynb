{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfebea9",
   "metadata": {},
   "source": [
    "# Grid search for hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29cbd35",
   "metadata": {},
   "source": [
    "# Import and setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31a22c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "# --- This assumes your model file 'moe_model.py' is in the same directory ---\n",
    "from moe_model import MoE as MoE_raw, MLP as MoE_Expert\n",
    "\n",
    "# --- Helper class to modify the MoE Expert to output raw logits ---\n",
    "# This is necessary because the CrossEntropyLoss function expects logits, not probabilities.\n",
    "class MoE_Expert_Logits(MoE_Expert):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super().__init__(input_size, output_size, hidden_size)\n",
    "        # Replace the final softmax layer with an identity layer\n",
    "        self.soft = nn.Identity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421feefa",
   "metadata": {},
   "source": [
    "# Trainning and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76af7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_trial(params, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a single trial of the MoE model with a given set of hyperparameters.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # --- Model Configuration ---\n",
    "    model = MoE_raw(\n",
    "        input_size=X_train.shape[1],\n",
    "        output_size=2,\n",
    "        num_experts=params['num_experts'],\n",
    "        hidden_size=params['hidden_size'],\n",
    "        k=params['k']\n",
    "    )\n",
    "    model.experts = nn.ModuleList([\n",
    "        MoE_Expert_Logits(\n",
    "            input_size=X_train.shape[1],\n",
    "            output_size=2,\n",
    "            hidden_size=params['hidden_size']\n",
    "        ) for _ in range(model.num_experts)\n",
    "    ])\n",
    "    model.to(device)\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.LongTensor(y_val).to(device)\n",
    "\n",
    "    # --- Training with Early Stopping ---\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_epochs = 75\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, aux_loss = model(X_train_tensor)\n",
    "        loss = criterion(y_pred, y_train_tensor) + aux_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred, val_aux_loss = model(X_val_tensor)\n",
    "            val_loss = criterion(y_val_pred, y_val_tensor) + val_aux_loss\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    # --- Final Evaluation on Validation Set ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor, _ = model(X_val_tensor)\n",
    "        probas = nn.functional.softmax(y_pred_tensor, dim=1)\n",
    "        _, predicted = torch.max(probas, 1)\n",
    "        \n",
    "        y_true = y_val_tensor.cpu().numpy()\n",
    "        y_pred = predicted.cpu().numpy()\n",
    "        y_score = probas[:, 1].cpu().numpy()\n",
    "        \n",
    "        # Get detailed metrics from classification report for class '1'\n",
    "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0).get('1', {})\n",
    "\n",
    "        return {\n",
    "            'num_experts': params['num_experts'],\n",
    "            'hidden_size': params['hidden_size'],\n",
    "            'k': params['k'],\n",
    "            'val_roc_auc': roc_auc_score(y_true, y_score),\n",
    "            'val_accuracy': accuracy_score(y_true, y_pred),\n",
    "            'val_f1_score': report.get('f1-score', 0),\n",
    "            'val_precision': report.get('precision', 0),\n",
    "            'val_recall': report.get('recall', 0),\n",
    "            'val_cohen_kappa': cohen_kappa_score(y_true, y_pred)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddad282",
   "metadata": {},
   "source": [
    "# Data Loading and Hyperparameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a74cca4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Loading and preparing data...\n",
      "Data loaded successfully. Train shape: (1297, 9)\n",
      "\n",
      "🔹 Starting Grid Search. Total combinations to test: 33\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load and Prepare Data ---\n",
    "try:\n",
    "    print(\"🔹 Loading and preparing data...\")\n",
    "    df = pd.read_csv('input_moe_original.csv', low_memory=False)\n",
    "    X = df.drop(columns=['Dementia Status'])\n",
    "    y = df['Dementia Status']\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Data loaded successfully. Train shape: {X_train.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n⚠️ ERROR: 'input_data.csv' not found. Please run the feature extractor first.\")\n",
    "    # In a notebook, you might want to stop execution here if the file is not found.\n",
    "\n",
    "# --- 2. Define the Hyperparameter Grid ---\n",
    "param_grid = {\n",
    "    'num_experts': [3, 4, 6, 8],\n",
    "    'hidden_size': [16, 32, 64],\n",
    "    'k': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create all possible combinations\n",
    "all_params = list(itertools.product(\n",
    "    param_grid['num_experts'],\n",
    "    param_grid['hidden_size'],\n",
    "    param_grid['k']\n",
    "))\n",
    "\n",
    "# Filter out invalid combinations where k > num_experts\n",
    "valid_params = [\n",
    "    {'num_experts': p[0], 'hidden_size': p[1], 'k': p[2]}\n",
    "    for p in all_params if p[2] <= p[0]\n",
    "]\n",
    "\n",
    "print(f\"\\n🔹 Starting Grid Search. Total combinations to test: {len(valid_params)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1969f",
   "metadata": {},
   "source": [
    "# Run grid search and final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c650b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Intermediate Tuning Results (Validation Set):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_experts</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>k</th>\n",
       "      <th>val_roc_auc</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_f1_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_cohen_kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.773412</td>\n",
       "      <td>0.715827</td>\n",
       "      <td>0.744337</td>\n",
       "      <td>0.684524</td>\n",
       "      <td>0.815603</td>\n",
       "      <td>0.429943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750997</td>\n",
       "      <td>0.715827</td>\n",
       "      <td>0.744337</td>\n",
       "      <td>0.684524</td>\n",
       "      <td>0.815603</td>\n",
       "      <td>0.429943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.757882</td>\n",
       "      <td>0.712230</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.678363</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.422548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.758710</td>\n",
       "      <td>0.708633</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.415147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.757002</td>\n",
       "      <td>0.708633</td>\n",
       "      <td>0.737864</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.415512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.758399</td>\n",
       "      <td>0.701439</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.400706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754983</td>\n",
       "      <td>0.701439</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.400706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.760056</td>\n",
       "      <td>0.701439</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.400706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.759745</td>\n",
       "      <td>0.701439</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.400706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.747373</td>\n",
       "      <td>0.697842</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.393297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.739504</td>\n",
       "      <td>0.694245</td>\n",
       "      <td>0.735202</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.836879</td>\n",
       "      <td>0.385883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750324</td>\n",
       "      <td>0.701439</td>\n",
       "      <td>0.733119</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.400955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.747476</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.732919</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.836879</td>\n",
       "      <td>0.378593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.758348</td>\n",
       "      <td>0.694245</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.660920</td>\n",
       "      <td>0.815603</td>\n",
       "      <td>0.386266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.730238</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.729560</td>\n",
       "      <td>0.655367</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.378852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.755345</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.815603</td>\n",
       "      <td>0.378981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750427</td>\n",
       "      <td>0.694245</td>\n",
       "      <td>0.726688</td>\n",
       "      <td>0.664706</td>\n",
       "      <td>0.801418</td>\n",
       "      <td>0.386521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.749754</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.658960</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.379110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.741575</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.724458</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.356849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.739245</td>\n",
       "      <td>0.683453</td>\n",
       "      <td>0.723270</td>\n",
       "      <td>0.649718</td>\n",
       "      <td>0.815603</td>\n",
       "      <td>0.364406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.734690</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.349555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754983</td>\n",
       "      <td>0.687050</td>\n",
       "      <td>0.722045</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>0.801418</td>\n",
       "      <td>0.371955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.745302</td>\n",
       "      <td>0.683453</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.364538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.738055</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.721003</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>0.815603</td>\n",
       "      <td>0.357117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.720505</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.719243</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.357250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.738779</td>\n",
       "      <td>0.683453</td>\n",
       "      <td>0.716129</td>\n",
       "      <td>0.656805</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.364934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.744215</td>\n",
       "      <td>0.687050</td>\n",
       "      <td>0.714754</td>\n",
       "      <td>0.664634</td>\n",
       "      <td>0.773050</td>\n",
       "      <td>0.372477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.656054</td>\n",
       "      <td>0.611511</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0.570213</td>\n",
       "      <td>0.950355</td>\n",
       "      <td>0.215223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.684734</td>\n",
       "      <td>0.651079</td>\n",
       "      <td>0.710448</td>\n",
       "      <td>0.613402</td>\n",
       "      <td>0.843972</td>\n",
       "      <td>0.298163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.730393</td>\n",
       "      <td>0.672662</td>\n",
       "      <td>0.709265</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.343080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750841</td>\n",
       "      <td>0.672662</td>\n",
       "      <td>0.703583</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.343489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.736657</td>\n",
       "      <td>0.672662</td>\n",
       "      <td>0.701639</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.758865</td>\n",
       "      <td>0.343625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.706554</td>\n",
       "      <td>0.633094</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>0.653543</td>\n",
       "      <td>0.588652</td>\n",
       "      <td>0.267097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_experts  hidden_size  k  val_roc_auc  val_accuracy  val_f1_score  \\\n",
       "10            4           32  3     0.773412      0.715827      0.744337   \n",
       "2             3           32  2     0.750997      0.715827      0.744337   \n",
       "4             3           64  2     0.757882      0.712230      0.743590   \n",
       "22            6           64  3     0.758710      0.708633      0.742857   \n",
       "12            4           64  2     0.757002      0.708633      0.737864   \n",
       "14            4           64  4     0.758399      0.701439      0.736508   \n",
       "13            4           64  3     0.754983      0.701439      0.736508   \n",
       "23            6           64  4     0.760056      0.701439      0.736508   \n",
       "32            8           64  4     0.759745      0.701439      0.736508   \n",
       "17            6           16  4     0.747373      0.697842      0.735849   \n",
       "31            8           64  3     0.739504      0.694245      0.735202   \n",
       "26            8           16  4     0.750324      0.701439      0.733119   \n",
       "24            8           16  2     0.747476      0.690647      0.732919   \n",
       "5             3           64  3     0.758348      0.694245      0.730159   \n",
       "28            8           32  3     0.730238      0.690647      0.729560   \n",
       "11            4           32  4     0.755345      0.690647      0.727848   \n",
       "29            8           32  4     0.750427      0.694245      0.726688   \n",
       "3             3           32  3     0.749754      0.690647      0.726115   \n",
       "1             3           16  3     0.741575      0.679856      0.724458   \n",
       "20            6           32  4     0.739245      0.683453      0.723270   \n",
       "21            6           64  2     0.734690      0.676259      0.722222   \n",
       "7             4           16  3     0.754983      0.687050      0.722045   \n",
       "30            8           64  2     0.745302      0.683453      0.721519   \n",
       "0             3           16  2     0.738055      0.679856      0.721003   \n",
       "25            8           16  3     0.720505      0.679856      0.719243   \n",
       "9             4           32  2     0.738779      0.683453      0.716129   \n",
       "19            6           32  3     0.744215      0.687050      0.714754   \n",
       "6             4           16  2     0.656054      0.611511      0.712766   \n",
       "15            6           16  2     0.684734      0.651079      0.710448   \n",
       "8             4           16  4     0.730393      0.672662      0.709265   \n",
       "27            8           32  2     0.750841      0.672662      0.703583   \n",
       "18            6           32  2     0.736657      0.672662      0.701639   \n",
       "16            6           16  3     0.706554      0.633094      0.619403   \n",
       "\n",
       "    val_precision  val_recall  val_cohen_kappa  \n",
       "10       0.684524    0.815603         0.429943  \n",
       "2        0.684524    0.815603         0.429943  \n",
       "4        0.678363    0.822695         0.422548  \n",
       "22       0.672414    0.829787         0.415147  \n",
       "12       0.678571    0.808511         0.415512  \n",
       "14       0.666667    0.822695         0.400706  \n",
       "13       0.666667    0.822695         0.400706  \n",
       "23       0.666667    0.822695         0.400706  \n",
       "32       0.666667    0.822695         0.400706  \n",
       "17       0.661017    0.829787         0.393297  \n",
       "31       0.655556    0.836879         0.385883  \n",
       "26       0.670588    0.808511         0.400955  \n",
       "24       0.651934    0.836879         0.378593  \n",
       "5        0.660920    0.815603         0.386266  \n",
       "28       0.655367    0.822695         0.378852  \n",
       "11       0.657143    0.815603         0.378981  \n",
       "29       0.664706    0.801418         0.386521  \n",
       "3        0.658960    0.808511         0.379110  \n",
       "1        0.642857    0.829787         0.356849  \n",
       "20       0.649718    0.815603         0.364406  \n",
       "21       0.639344    0.829787         0.349555  \n",
       "7        0.656977    0.801418         0.371955  \n",
       "30       0.651429    0.808511         0.364538  \n",
       "0        0.646067    0.815603         0.357117  \n",
       "25       0.647727    0.808511         0.357250  \n",
       "9        0.656805    0.787234         0.364934  \n",
       "19       0.664634    0.773050         0.372477  \n",
       "6        0.570213    0.950355         0.215223  \n",
       "15       0.613402    0.843972         0.298163  \n",
       "8        0.645349    0.787234         0.343080  \n",
       "27       0.650602    0.765957         0.343489  \n",
       "18       0.652439    0.758865         0.343625  \n",
       "16       0.653543    0.588652         0.267097  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "✅ FINAL EVALUATION ON HELD-OUT TEST SET\n",
      "========================================\n",
      "🏆 Best Hyperparameters found (based on validation F1-score):\n",
      "  - Number of Experts: 4\n",
      "  - Hidden Size: 32\n",
      "  - Top K: 3\n",
      "\n",
      "Retraining the best model on combined Train+Validation data...\n",
      "\n",
      "--- Final Performance on Test Set ---\n",
      "  - Accuracy:    0.7518\n",
      "  - ROC-AUC:     0.7959\n",
      "  - F1-Score:    0.7781\n",
      "  - Precision:   0.7076\n",
      "  - Recall:      0.8643\n",
      "  - Cohen Kappa: 0.5028\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Run the Grid Search ---\n",
    "results = []\n",
    "for i, params in enumerate(valid_params):\n",
    "    print(f\"\\n--- Testing Combination {i+1}/{len(valid_params)} ---\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "    \n",
    "    result = train_and_evaluate_trial(params, X_train_scaled, y_train.values, X_val_scaled, y_val.values)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Live update of results\n",
    "    clear_output(wait=True)\n",
    "    # --- CHANGE: Sort by F1-score ---\n",
    "    results_df = pd.DataFrame(results).sort_values('val_f1_score', ascending=False)\n",
    "    print(\"✅ Intermediate Tuning Results (Validation Set):\")\n",
    "    display(results_df)\n",
    "\n",
    "# --- 4. Final Evaluation on Test Set ---\n",
    "print(\"\\n\\n\" + \"=\"*40 + \"\\n✅ FINAL EVALUATION ON HELD-OUT TEST SET\\n\" + \"=\"*40)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = results_df.iloc[0].to_dict()\n",
    "print(\"🏆 Best Hyperparameters found (based on validation F1-score):\")\n",
    "print(f\"  - Number of Experts: {int(best_params['num_experts'])}\")\n",
    "print(f\"  - Hidden Size: {int(best_params['hidden_size'])}\")\n",
    "print(f\"  - Top K: {int(best_params['k'])}\")\n",
    "\n",
    "# Combine training and validation data\n",
    "X_train_val = np.concatenate((X_train_scaled, X_val_scaled), axis=0)\n",
    "y_train_val = np.concatenate((y_train.values, y_val.values), axis=0)\n",
    "\n",
    "print(\"\\nRetraining the best model on combined Train+Validation data...\")\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "final_model = MoE_raw(\n",
    "    input_size=X_train_val.shape[1],\n",
    "    output_size=2,\n",
    "    num_experts=int(best_params['num_experts']),\n",
    "    hidden_size=int(best_params['hidden_size']),\n",
    "    k=int(best_params['k'])\n",
    ")\n",
    "final_model.experts = nn.ModuleList([\n",
    "    MoE_Expert_Logits(\n",
    "        input_size=X_train_val.shape[1],\n",
    "        output_size=2,\n",
    "        hidden_size=int(best_params['hidden_size'])\n",
    "    ) for _ in range(final_model.num_experts)\n",
    "])\n",
    "\n",
    "# Train the final model and evaluate on the test set\n",
    "final_results = train_and_evaluate_trial(\n",
    "    {'num_experts': int(best_params['num_experts']), 'hidden_size': int(best_params['hidden_size']), 'k': int(best_params['k'])},\n",
    "    X_train_val, y_train_val, X_test_scaled, y_test.values\n",
    ")\n",
    "\n",
    "# --- CHANGE: Print all requested metrics for the final evaluation ---\n",
    "print(\"\\n--- Final Performance on Test Set ---\")\n",
    "print(f\"  - Accuracy:    {final_results['val_accuracy']:.4f}\")\n",
    "print(f\"  - ROC-AUC:     {final_results['val_roc_auc']:.4f}\")\n",
    "print(f\"  - F1-Score:    {final_results['val_f1_score']:.4f}\")\n",
    "print(f\"  - Precision:   {final_results['val_precision']:.4f}\")\n",
    "print(f\"  - Recall:      {final_results['val_recall']:.4f}\")\n",
    "print(f\"  - Cohen Kappa: {final_results['val_cohen_kappa']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeab97",
   "metadata": {},
   "source": [
    "We conducted a systematic hyperparameter tuning process for the Mixture of Experts (MoE) model using a Grid Search to find the optimal architecture for our dementia prediction task. This involved exhaustively testing every possible combination of key parameters, including the number of experts, the hidden size of each expert, and the number of top experts to use for each prediction. For every combination, a new model was trained on the training data while its performance was monitored on the validation set to find the best F1-score (based on rfe) and prevent overfitting through early stopping. After completing this comprehensive search, the single best-performing set of hyperparameters was identified, and a final, optimized model was retrained on the combined training and validation data before being evaluated on the held-out test set to produce a final, unbiased measure of its predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
