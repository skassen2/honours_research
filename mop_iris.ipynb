{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83737cd4",
   "metadata": {},
   "source": [
    "# Mixture-of-Pathways model.\n",
    "See \"Brain-Like Processing Pathways Form in Models With Heterogeneous Experts\"\n",
    "https://arxiv.org/pdf/2506.02813\n",
    "\n",
    "Author: Jack Cook Danyal Akarca Rui Ponte Costa, Jascha Achterberg\n",
    "\n",
    "The code is based on the implementation:\n",
    "https://anonymous.4open.science/r/mixture-of-pathways-anon-48BA/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57a05f",
   "metadata": {},
   "source": [
    "# Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "import warnings\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7fb8af",
   "metadata": {},
   "source": [
    "# Model configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaea512",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MoPConfig:\n",
    "    \"\"\"Configuration class for the Mixture of Pathways model.\"\"\"\n",
    "    input_dim: int = 4\n",
    "    output_dim: int = 3\n",
    "    intermediate_dim: int = 64\n",
    "    # Defines 3 MoE layers, each with 3 experts (0=identity, 16/32=hidden_dim)\n",
    "    layers: List[str] = (\"0,16,32\", \"0,16,32\", \"0,16,32\")\n",
    "    task_id: str = \"iris\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    task_dim: int = 10\n",
    "    expert_cost_exponent: float = 2.0\n",
    "    # Disable complex dropout/noise features for this simple task\n",
    "    within_expert_dropout_prob: Optional[float] = None\n",
    "    routing_weight_noise: Optional[float] = None\n",
    "    dropout_max_prob: Optional[float] = None\n",
    "    dropout_router_weight_threshold: Optional[float] = None\n",
    "    flat_expert_knockout_prob: Optional[float] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b7e0df",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MoPConfig,\n",
    "        hidden_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.identity = hidden_dim == 0\n",
    "        if not self.identity:\n",
    "            self.rnn = nn.GRU(config.intermediate_dim, hidden_dim)\n",
    "            self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = (\n",
    "                nn.Dropout(config.within_expert_dropout_prob)\n",
    "                if config.within_expert_dropout_prob is not None\n",
    "                else nn.Identity()\n",
    "            )\n",
    "            self.output_layer = nn.Linear(hidden_dim, config.intermediate_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.identity:\n",
    "            return x\n",
    "        else:\n",
    "            # Reshape for RNN: (batch*seq_len, features) -> (seq_len, batch, features)\n",
    "            if x.dim() == 2:\n",
    "                x = x.unsqueeze(0)\n",
    "\n",
    "            x, _ = self.rnn(x)\n",
    "\n",
    "            # Reshape back: (seq_len, batch, features) -> (batch*seq_len, features)\n",
    "            if x.dim() == 3:\n",
    "                 x = x.squeeze(0)\n",
    "\n",
    "            if x.shape[0] > 1 and x.var() != 0:\n",
    "                 x = self.batchnorm(x)\n",
    "\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.output_layer(x)\n",
    "            return x\n",
    "\n",
    "class CostBasedRouter(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MoPConfig,\n",
    "        expert_dims: List[int],\n",
    "        num_tasks: int,\n",
    "    ):\n",
    "        super(CostBasedRouter, self).__init__()\n",
    "        self.config = config\n",
    "        self.expert_dims = expert_dims\n",
    "        self.num_tasks = num_tasks\n",
    "        self.rnn = nn.GRU(config.intermediate_dim, config.intermediate_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(config.intermediate_dim, len(expert_dims))\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", dtype=torch.float32)\n",
    "    def forward(\n",
    "        self,\n",
    "        prev_layer_output: torch.Tensor,\n",
    "        task_ids: torch.Tensor,\n",
    "        *,\n",
    "        inference: bool = False,\n",
    "        inference_dropout_threshold: Optional[float] = None,\n",
    "        inference_disable_complex_experts: bool = False,\n",
    "    ):\n",
    "        logits, _ = self.rnn(prev_layer_output)\n",
    "        logits = self.relu(logits)\n",
    "        logits = self.output_layer(logits)\n",
    "        indices = torch.zeros(\n",
    "            (logits.shape[0], logits.shape[1], len(self.expert_dims)),\n",
    "            dtype=torch.long,\n",
    "            device=logits.device,\n",
    "        )\n",
    "        for i in range(len(self.expert_dims)):\n",
    "            indices[:, :, i] = i\n",
    "        raw_router_output = F.softmax(logits, dim=-1)\n",
    "        router_output = raw_router_output.clone()\n",
    "        # Simplified: Skipping inference and complex dropout logic for this task\n",
    "        if len(self.expert_dims) > 1:\n",
    "            routing_costs = torch.tensor(\n",
    "                [\n",
    "                    expert_dim**self.config.expert_cost_exponent\n",
    "                    for expert_dim in self.expert_dims\n",
    "                ],\n",
    "                dtype=logits.dtype,\n",
    "                device=logits.device,\n",
    "            )\n",
    "            task_expert_usage_losses = {}\n",
    "            expert_usage_loss = torch.einsum(\n",
    "                \"ijk,k->ij\", raw_router_output, routing_costs\n",
    "            )\n",
    "            for i in range(self.num_tasks):\n",
    "                task_mask = task_ids == i\n",
    "                if task_mask.sum() > 0:\n",
    "                    task_expert_usage_losses[i] = (\n",
    "                        expert_usage_loss[task_mask].sum() / task_mask.sum()\n",
    "                    )\n",
    "                else:\n",
    "                    task_expert_usage_losses[i] = torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "            expert_entropy_loss = (\n",
    "                -torch.sum(raw_router_output * torch.log(raw_router_output + 1e-10))\n",
    "                / raw_router_output.nelement()\n",
    "            )\n",
    "        else:\n",
    "            task_expert_usage_losses = None\n",
    "            expert_entropy_loss = None\n",
    "        return (\n",
    "            raw_router_output,\n",
    "            router_output,\n",
    "            indices,\n",
    "            task_expert_usage_losses,\n",
    "            expert_entropy_loss,\n",
    "        )\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MoPConfig,\n",
    "        expert_dims: List[int],\n",
    "        num_tasks: int,\n",
    "    ):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = CostBasedRouter(config, expert_dims, num_tasks)\n",
    "        self.experts = nn.ModuleList(\n",
    "            [Expert(config, expert_dim) for expert_dim in expert_dims]\n",
    "        )\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        task_ids: torch.Tensor,\n",
    "        *,\n",
    "        inference: bool = False,\n",
    "        inference_dropout_threshold: Optional[float] = None,\n",
    "        inference_disable_complex_experts: bool = False,\n",
    "        output_activations: bool = False,\n",
    "    ):\n",
    "        (\n",
    "            raw_router_output,\n",
    "            router_output,\n",
    "            indices,\n",
    "            task_expert_usage_losses,\n",
    "            expert_entropy_loss,\n",
    "        ) = self.router(\n",
    "            x,\n",
    "            task_ids,\n",
    "            inference=inference,\n",
    "        )\n",
    "        final_output = torch.zeros_like(x)\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_router_output = router_output.view(-1, router_output.size(-1))\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "            if not flat_mask.any():\n",
    "                continue\n",
    "\n",
    "            expert_input = flat_x[flat_mask]\n",
    "            expert_output = expert(expert_input)\n",
    "\n",
    "            gating_scores = flat_router_output[flat_mask, i]\n",
    "            weighting_output = torch.einsum(\"i,ij->ij\", gating_scores, expert_output)\n",
    "\n",
    "            # Explicitly cast weighting_output to the same dtype as final_output\n",
    "            final_output.view(-1, final_output.size(-1)).index_add_(0, torch.where(flat_mask)[0], weighting_output.to(final_output.dtype))\n",
    "\n",
    "        return (\n",
    "            final_output,\n",
    "            raw_router_output,\n",
    "            task_expert_usage_losses,\n",
    "            expert_entropy_loss,\n",
    "        )\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MoPConfig,\n",
    "        expert_dims: List[int],\n",
    "        num_tasks: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sparse_moe = SparseMoE(config, expert_dims, num_tasks)\n",
    "        self.ln = nn.LayerNorm(config.intermediate_dim)\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        task_ids: torch.Tensor,\n",
    "        *,\n",
    "        inference: bool = False,\n",
    "        inference_dropout_threshold: Optional[float] = None,\n",
    "        inference_disable_complex_experts: bool = False,\n",
    "        output_activations: bool = False,\n",
    "    ):\n",
    "        moe_outputs = self.sparse_moe(\n",
    "            x, task_ids, inference=inference\n",
    "        )\n",
    "        # Add post-layer norm (residual connection)\n",
    "        moe_outputs = (self.ln(x + moe_outputs[0]), *moe_outputs[1:])\n",
    "        return moe_outputs\n",
    "\n",
    "class MoPModel(nn.Module):\n",
    "    def __init__(self, config: MoPConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # For Iris, we have only 1 \"task\"\n",
    "        self.num_tasks = 1 if config.task_id == \"iris\" else 82\n",
    "        self.device = torch.device(config.device)\n",
    "        self.input_layer = nn.Linear(\n",
    "            self.config.input_dim, self.config.intermediate_dim\n",
    "        )\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    self.config,\n",
    "                    [int(expert_size) for expert_size in layer.split(\",\")],\n",
    "                    self.num_tasks,\n",
    "                )\n",
    "                for layer in self.config.layers\n",
    "            ]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(\n",
    "            self.config.intermediate_dim, self.config.output_dim\n",
    "        )\n",
    "        self.to(self.config.device)\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=True)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # For Iris (single task), task_ids are all zeros.\n",
    "        task_ids = torch.zeros(x.shape[0], x.shape[1], dtype=torch.long, device=self.device)\n",
    "        x = self.input_layer(x)\n",
    "        total_task_expert_usage_losses = {\n",
    "            i: torch.tensor(0.0, device=self.device) for i in range(self.num_tasks)\n",
    "        }\n",
    "        total_expert_entropy_loss = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x, _, task_expert_usage_losses, expert_entropy_loss = block(x, task_ids)\n",
    "            if task_expert_usage_losses is not None:\n",
    "                for k in task_expert_usage_losses:\n",
    "                    total_task_expert_usage_losses[k] = (\n",
    "                        total_task_expert_usage_losses[k] + task_expert_usage_losses[k]\n",
    "                    )\n",
    "            if expert_entropy_loss is not None:\n",
    "                total_expert_entropy_loss += expert_entropy_loss\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x, total_task_expert_usage_losses, total_expert_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bc5de",
   "metadata": {},
   "source": [
    "# Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iris_dataloader(batch_size=32, test_size=0.2):\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create PyTorch datasets and dataloaders\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def run_experiment():\n",
    "    # Hyperparameters\n",
    "    config = MoPConfig()\n",
    "    epochs = 100\n",
    "    lr = 0.001\n",
    "    batch_size = 16\n",
    "    # Coefficients for the auxiliary losses\n",
    "    usage_loss_coeff = 0.01\n",
    "    entropy_loss_coeff = 0.001\n",
    "\n",
    "    print(\"--- Configuration ---\")\n",
    "    print(f\"Device: {config.device}\")\n",
    "    print(f\"Epochs: {epochs}, Batch Size: {batch_size}, Learning Rate: {lr}\")\n",
    "    print(f\"Model Layers: {config.layers}\")\n",
    "    print(\"-\" * 21)\n",
    "\n",
    "    # Get data\n",
    "    train_loader, test_loader = get_iris_dataloader(batch_size)\n",
    "\n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = MoPModel(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    classification_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #lists for graphs\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # --- Training & Validation Loop ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        train_loss, epoch_usage_loss, epoch_entropy_loss = 0.0, 0.0, 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.unsqueeze(1).to(config.device)\n",
    "            y_batch = y_batch.to(config.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, usage_losses, entropy_loss = model(X_batch)\n",
    "            y_pred = y_pred.squeeze(1)\n",
    "\n",
    "            class_loss = classification_criterion(y_pred, y_batch)\n",
    "            total_usage_loss = usage_losses[0] # Single task, so we get the first loss\n",
    "            total_loss = class_loss + (usage_loss_coeff * total_usage_loss) + (entropy_loss_coeff * entropy_loss)\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += class_loss.item()\n",
    "            epoch_usage_loss += total_usage_loss.item()\n",
    "            epoch_entropy_loss += entropy_loss.item()\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.unsqueeze(1).to(config.device)\n",
    "                y_batch = y_batch.to(config.device)\n",
    "\n",
    "                y_pred, _, _ = model(X_batch)\n",
    "                y_pred = y_pred.squeeze(1)\n",
    "                loss = classification_criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # --- Print Epoch Results ---\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        avg_usage_loss = epoch_usage_loss / len(train_loader)\n",
    "        avg_entropy_loss = epoch_entropy_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1:02d}/{epochs:02d}] | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "            f\"Aux Usage: {avg_usage_loss:.2f} | \"\n",
    "            f\"Aux Entropy: {avg_entropy_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"--- Training Complete ---\")\n",
    "\n",
    "    # --- Final Accuracy Evaluation ---\n",
    "    print(\"\\n--- Evaluating Final Accuracy on Test Set ---\")\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.unsqueeze(1).to(config.device)\n",
    "            y_batch = y_batch.to(config.device)\n",
    "\n",
    "            y_pred, _, _ = model(X_batch)\n",
    "            y_pred = y_pred.squeeze(1)\n",
    "\n",
    "            _, predicted_labels = torch.max(y_pred, 1)\n",
    "            total_samples += y_batch.size(0)\n",
    "            total_correct += (predicted_labels == y_batch).sum().item()\n",
    "\n",
    "    accuracy = (total_correct / total_samples) * 100\n",
    "    print(f\"\\nFinal Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
